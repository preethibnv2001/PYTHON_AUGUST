{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-9KE6MmRd8Ov"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\",\"RDD_revision\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = [1,2,3,4,5,6,7,8,9,]\n",
        "print(type(list1))\n",
        "\n",
        "rdd1 = sc.parallelize(list1)\n",
        "print(type(rdd1))\n",
        "\n",
        "# parallelize turns a normal Python list into an RDD so Spark can process it in parallel."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMGtk7oPe9Cp",
        "outputId": "72d9edb8-8fd2-4031-fe2d-37e759ad5626"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RDD - (Resilient Distributed Dataset)\n",
        "# In PySpark, an RDD (Resilient Distributed Dataset) is the fundamental low-level data structure used for distributed processing.\n",
        "# While most people now use DataFrames for convenience and optimization, RDDs are still important for:\n",
        "\n",
        "# Low-level transformations\n",
        "\n",
        "# Fine-grained control\n",
        "\n",
        "# Working with unstructured data\n",
        "\n",
        "# Functional-style distributed computing\n",
        "\n",
        "\n",
        "\n",
        "# ‚úÖ What is an RDD?\n",
        "\n",
        "# An RDD is an immutable, distributed collection of elements that can be processed in parallel.\n",
        "\n",
        "# Key characteristics:\n",
        "\n",
        "# Immutable ‚Äì once created, it cannot change\n",
        "\n",
        "# Lazy evaluation ‚Äì transformations are executed only when an action is called\n",
        "\n",
        "# Fault-tolerant ‚Äì can be recomputed via lineage\n",
        "\n"
      ],
      "metadata": {
        "id": "VewxH-RYf4MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new rdd as >=5 from rdd1\n",
        "\n",
        "rdd2 = rdd1.filter(lambda x:x>=5)\n",
        "rdd2.collect()\n",
        "\n",
        "#In PySpark, the filter() transformation on an RDD (Resilient Distributed Dataset)\n",
        "#is used to create a new RDD containing only the elements that satisfy a specified condition.\n",
        "# This condition is provided as a function (often a lambda function) that takes an element of\n",
        "#the RDD as input and returns True if the element should be included in the new RDD,\n",
        "#and False otherwise.\n",
        "\n",
        "\n",
        "#In PySpark, the collect() action on an RDD (Resilient Distributed Dataset)\n",
        "#is used to retrieve all the elements of the RDD and return them as a list to the driver program."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFR7WqFPgHmP",
        "outputId": "543b6be1-8724-42e3-d118-90594d190f1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create new rdd by adding 10 to each value in rdd1\n",
        "\n",
        "rdd2 = rdd1.map(lambda a : a+10)\n",
        "rdd2.collect()\n",
        "\n",
        "#In PySpark, the map transformation on an RDD (Resilient Distributed Dataset)\n",
        "#is a fundamental operation used to apply a function to each element of the RDD,\n",
        "# resulting in a new RDD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR-jvlMPgHUa",
        "outputId": "c94d0d06-23cc-4bfe-be9d-fd40cadabc84"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11, 12, 13, 14, 15, 16, 17, 18, 19]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data =['pyspark session is going on', 'rdd concept started']\n",
        "rdd1= sc.parallelize(data)\n",
        "print( ' === flatMap output ====')\n",
        "rdd3 = rdd1.flatMap( lambda x: x.split())\n",
        "rdd3.saveAsTextFile(\"results\")\n",
        "\n",
        "\n",
        "# ‚úÖ Meaning of flatMap in PySpark\n",
        "\n",
        "# flatMap takes each element in an RDD and maps it to multiple elements, then flattens the results into one RDD.\n",
        "\n",
        "# In even simpler words:\n",
        "\n",
        "# üëâ map gives one output per input\n",
        "# üëâ flatMap gives many outputs per input\n",
        "# üëâ and then flattens them into a single list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io5C5clEhXyp",
        "outputId": "efe734f5-5dd8-4bcf-e0c3-2f758aa1b378"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " === flatMap output ====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list1 = [54,56,6,6,1,2,3,4]\n",
        "# rdd= sc.parallelize(list1)\n",
        "\n",
        "# # rdd.count()\n",
        "# rdd.last()\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "# last_element = rdd.collect().last\n",
        "last_line = rdd.zipWithIndex().max()[0]\n",
        "\n",
        "print(last_line)\n",
        "\n",
        "\n",
        "# zipWithIndex pairs each element of an RDD with its index (position), starting from 0.\n",
        "# This line finds and returns the last element of the RDD.\n",
        "# max() returns the largest element in an RDD.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yKP3RJ8HvGq",
        "outputId": "54f0626d-48a2-4eb6-ae3a-ceeb6c7a3662"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 =[1,3,4,5,2,3,4,1,2,5,6]\n",
        "\n",
        "rdd= sc.parallelize(list1)\n",
        "\n",
        "rdd2 =rdd.sortBy(lambda x:x, ascending= False)\n",
        "rdd2.collect()\n",
        "\n",
        "# sortBy sorts an RDD based on a key you choose.\n",
        "\n",
        "# You give it a function, and Spark sorts the RDD using the value returned by that function\n",
        "\n",
        "# lambda creates a small, anonymous function without a name."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT1OBjqvH8pT",
        "outputId": "aeb4a295-fdcf-4474-d1b8-80ea0ed0b526"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 5, 5, 4, 4, 3, 3, 2, 2, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = [1,2,3,4,5]\n",
        "\n",
        "rdd = sc.parallelize(list1)\n",
        "\n",
        "res= rdd.fold(0, lambda x, y : x+y)\n",
        "\n",
        "print(res)\n",
        "print(type(res))\n",
        "\n",
        "# fold combines all elements of an RDD using the same operation, starting with a ‚Äúzero value.‚Äù\n",
        "\n",
        "# It is like reduce, but with an initial value added to every partition."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWIGJLnGIBUY",
        "outputId": "d4b083a7-6769-4a67-f879-ea1a0cd513b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "<class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd= sc.textFile(\"prac.txt\")\n",
        "\n",
        "rdd.collect()\n",
        "\n",
        "# textFile reads a text file and returns an RDD where each element is one line of the file."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m3Jsg99IFEL",
        "outputId": "b8982c92-4487-44a7-e465-5ef88a414a4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a b', 'c ', 'd', 'e f', 'g h']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd= sc.textFile(\"travel.csv\")\n",
        "\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3VhPaT2IH6A",
        "outputId": "3c20530d-b780-40d4-b256-195d5d622f44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cust_id,flight_id,origin,destination,price',\n",
              " '1,f1,delhi,hyderabad,2500',\n",
              " '1,f2,hyderabad,kochi,1700',\n",
              " '1,f3,kochi,Mangalore,1800',\n",
              " '2,f1,Mumbai,Ayodhya,4000',\n",
              " '2,f2,Ayodhya,chennai,3000',\n",
              " ' ']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = ['hello', 'world', 'students']\n",
        "\n",
        "rdd= sc.parallelize(list1)\n",
        "\n",
        "rdd.take(2)\n",
        "\n",
        "# take(n) returns the first n elements of an RDD."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhZGvoDJ-tr",
        "outputId": "c8dd85dc-b3e4-4a26-8525-1d07d7accd8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'world']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = ['hello', 'world', 'students', 'hello', 'world', 'hello', 'hello', 'world', 'students']\n",
        "rdd= sc.parallelize(list1)\n",
        "\n",
        "rdd1 = rdd.map( lambda x : (x,1))\n",
        "rdd2 = rdd1.reduceByKey(lambda x, y: x+y)\n",
        "rdd2.collect()\n",
        "\n",
        "# reduceByKey groups values by key and then reduces (combines) the values of each key using the function you provide"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR2pq5veJ-hk",
        "outputId": "5bb94608-f649-42b7-8917-92aec1f23100"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hello', 4), ('world', 3), ('students', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sk1LONjdJ-V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ISMJErTJ-Jt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}