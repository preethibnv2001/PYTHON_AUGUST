{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bTsD845wC-vB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"day1\").getOrCreate()\n",
        "\n",
        "data = ((1, 'Charan'),\n",
        "        (2, 'Bharath'),\n",
        "        (3, 'Dheeraj'))\n",
        "columns = ['id', 'name']\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.printSchema()\n",
        "df.show()\n",
        "\n",
        "\n",
        "# It creates a DataFrame (a tabular, distributed data structure) from local data or an RDD.\n",
        "\n",
        "# A Spark DataFrame is similar to a table in SQL or a DataFrame in Pandas, but distributed across a cluster.\n",
        "\n",
        "# df.printSchema() and df.show() are two common PySpark DataFrame methods used to inspect data.\n",
        "\n",
        "# 1. df.printSchema() — Shows the structure of the DataFrame\n",
        "\n",
        "# This prints:\n",
        "\n",
        "# Column names\n",
        "\n",
        "# Data types\n",
        "\n",
        "# Whether values are nullable\n",
        "\n",
        "# 2.df.show() — Displays the DataFrame rows\n",
        "\n",
        "# This prints the actual data, like a table.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j9Vg5HuPB8p",
        "outputId": "2d98b458-ba49-4365-a19a-25871f215b02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n",
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| Charan|\n",
            "|  2|Bharath|\n",
            "|  3|Dheeraj|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data =((1,'charan'),\n",
        "       (2, 'Bharat'),\n",
        "       (3, 'dheeraj'))\n",
        "\n",
        "columns = ['id', 'name']\n",
        "\n",
        "df1= spark.createDataFrame(data, columns)\n",
        "\n",
        "df1.printSchema()\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "df2= rdd.toDF()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPAsxjDFP3wm",
        "outputId": "e683aaa8-3a78-44a8-efa7-3293ff105a2c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "schema = StructType ( [\n",
        "    StructField (\"id\", IntegerType(), False),\n",
        "    StructField (\"name\", StringType(), True),\n",
        "])\n",
        "\n",
        "df3 = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "df3.printSchema()\n",
        "\n",
        "\n",
        "# 1. StructType\n",
        "\n",
        "# Represents the overall schema (a collection of fields, like a table structure).\n",
        "\n",
        "# 2. StructField\n",
        "\n",
        "# Represents one column inside the schema with:\n",
        "\n",
        "# name\n",
        "\n",
        "# data type\n",
        "\n",
        "# nullable flag\n",
        "\n",
        "# 3. StringType\n",
        "\n",
        "# Column type → string\n",
        "\n",
        "# 4. IntegerType\n",
        "\n",
        "# Column type → integer\n",
        "\n",
        "# 5. DoubleType\n",
        "\n",
        "# Column type → double/float"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqqf9oMWP3ku",
        "outputId": "56f5406d-eeee-4c10-9dc3-149cdf5c817b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = false)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = StructType ( [\n",
        "    StructField (\"id\", IntegerType(), False),\n",
        "    StructField (\"name\", StringType(), False),\n",
        "])\n",
        "\n",
        "csv_df = spark.read.format(\"csv\").option(\"header\", 'true').schema(columns).load(\"source.csv\")\n",
        "csv_df.show()\n",
        "csv_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdT6bWZhQQkM",
        "outputId": "94133023-926d-4dc9-b179-00bfac733d1a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+\n",
            "|  id|name|\n",
            "+----+----+\n",
            "|NULL|NULL|\n",
            "|   2|   b|\n",
            "|   3|   c|\n",
            "|   4|   d|\n",
            "|   1|   a|\n",
            "+----+----+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### assignment\n",
        "\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "data = [\n",
        "    (\"charan\", 1000),\n",
        "    (\"bharath\", 1000),\n",
        "    (\"charan\", 500),\n",
        "    (\"bharath\", 2000),\n",
        "    (\"hari\", 9000),\n",
        "    (\"vikram\", 1500)\n",
        "]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "result_rdd = rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(result_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gt6xD1IRutS",
        "outputId": "faf75b1a-333b-4ac1-badc-0fe3d2f56849"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('charan', 1500), ('bharath', 3000), ('hari', 9000), ('vikram', 1500)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_rdd.sortByKey().collect())\n",
        "# output sorted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtBZWzQkUytD",
        "outputId": "5e70a165-9f00-4cee-8bf6-cec784bfc137"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bharath', 3000), ('charan', 1500), ('hari', 9000), ('vikram', 1500)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9wBs0YIVA-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}